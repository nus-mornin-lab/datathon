{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hxAt4I3hfrfo"
   },
   "source": [
    "# Understanding Electronic Health Records with BigQuery ML\n",
    "\n",
    "This tutorial introduces\n",
    "[BigQuery ML](https://cloud.google.com/bigquery/docs/bigqueryml) (BQML) in the\n",
    "context of working with the [MIMIC3](https://mimic.physionet.org/about/mimic/)\n",
    "dataset.\n",
    "\n",
    "BigQuery ML adds only a few statements to\n",
    "[standard SQL](https://cloud.google.com/bigquery/docs/reference/standard-sql/).\n",
    "These statements automate the creation and evaluation of statistical models on\n",
    "BigQuery datasets. BigQuery ML has several\n",
    "[advantages](https://cloud.google.com/bigquery/docs/bigqueryml-intro#advantages_of)\n",
    "over older machine learning tools and workflows. Some highlights are BQML's high\n",
    "performance on massive datasets, support for\n",
    "[HIPAA compliance](https://cloud.google.com/security/compliance/hipaa/), and\n",
    "ease of use. BQML automatically implements state of the art best practices in\n",
    "machine learning for your dataset.\n",
    "\n",
    "MIMIC3 is a 10-year database of health records from the intensive care unit of\n",
    "Beth Israel Deaconess Medical Center in Boston. It's full of insights that are\n",
    "just begging to be uncovered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "10P1z6tyB6My"
   },
   "source": [
    "### Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H9M_kiQIByYL"
   },
   "source": [
    "[**Setup**](#scrollTo=CpGhZ7GuiSy_)\n",
    "\n",
    "Covers importing libraries, and authenticating with Google Cloud in Colab.\n",
    "\n",
    "[**Case complexity & mortality**](#scrollTo=Pg9AQy00xfPf)\n",
    "\n",
    "Non-technical. Introduces the theme for this tutorial.\n",
    "\n",
    "[**Taking a first look at the data**](#scrollTo=j02wTJnmfUGS)\n",
    "\n",
    "Covers basic SQL syntax, how BigQuery integrates with Colab and pandas, and the\n",
    "basics of creating visualizations with seaborn.\n",
    "\n",
    "[**Creating a classification model**](#scrollTo=OABwzKZAjXJI)\n",
    "\n",
    "Covers creating and training simple models with BigQuery ML.\n",
    "\n",
    "[**Plotting the predictions** ](#scrollTo=73Z0dBnTE524)\n",
    "\n",
    "Covers inference (making predictions) with BigQuery ML models, and how to\n",
    "inspect the weights of a parametric model.\n",
    "\n",
    "[**Adding a confounding variable**](#scrollTo=Pg9AQy00xfPf)\n",
    "\n",
    "Covers creating and training a slightly more complicated model, and introduces\n",
    "how BigQuery ML's model comparison features can be used to address confounding\n",
    "relationships.\n",
    "\n",
    "[**Plotting ROC and precision-recall curves**](#scrollTo=moxWSHVj6PzN&line=40&uniqifier=1)\n",
    "\n",
    "Covers how to create ROC and precision-recall curves with BigQuery ML. These are\n",
    "visualizations that describe the performance of binary classification models .\n",
    "\n",
    "[**More complex models**](#scrollTo=o52gB0FEVCw_)\n",
    "\n",
    "[**Creating the models**](#scrollTo=up-C-eU-6SAX&line=2&uniqifier=1)\n",
    "\n",
    "Covers creating logistic regression models with many input variables.\n",
    "\n",
    "[**Getting evaluation metrics**](#scrollTo=o52gB0FEVCw_)\n",
    "\n",
    "Covers how to get numerical measures of model performance using BigQuery ML.\n",
    "\n",
    "[**Exploring our model** ](#scrollTo=E-JEGEIs4xyM)\n",
    "\n",
    "Demonstrates how to interpret models with many variables.\n",
    "\n",
    "[**Conclusion**](#scrollTo=WHkTRwmpQtXI)\n",
    "\n",
    "Non-technical. Looks back on how we have used BigQuery ML to answer a research\n",
    "question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CpGhZ7GuiSy_"
   },
   "source": [
    "## Setup\n",
    "\n",
    "First, you'll need to sign into your google account to access the Google Cloud\n",
    "Platform (GCP).\n",
    "\n",
    "We're also going to import some standard python data analysis packages that\n",
    "we'll use later to visualize our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xjXC2JIllQ-u"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from google.colab import auth\n",
    "from google.cloud import bigquery\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fi8dtyTElS9x"
   },
   "outputs": [],
   "source": [
    "auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5cNVWKJaBK8g"
   },
   "source": [
    "Next you'll need to enter some information on how to access the data.\n",
    "\n",
    "`analysis_project` is the project used for processing the queries.\n",
    "\n",
    "The other fields,\n",
    "[admissions_table](https://mimic.physionet.org/mimictables/admissions/),\n",
    "[d_icd_diagnoses_table](https://mimic.physionet.org/mimictables/d_icd_diagnoses/),\n",
    "[diagnoses_icd_table](https://mimic.physionet.org/mimictables/diagnoses_icd/),\n",
    "and [patients_table](https://mimic.physionet.org/mimictables/patients/),\n",
    "identify the BigQuery tables we're going to query. They're written in the form\n",
    "`\"project_id.dataset_id.table_id\"`. We're going to use a slightly modified\n",
    "version of the `%%bigquery` cell magic in this tutorial, which replaces these\n",
    "variables with their values whenever they're surrounded by curly-braces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "141lb7WilVRK"
   },
   "outputs": [],
   "source": [
    "#@title Fill out this form then press [shift ⇧]+[enter ⏎] {run: \"auto\"}\n",
    "import subprocess\n",
    "import re\n",
    "\n",
    "analysis_project = 'singapore-datathon-team'  #@param {type:\"string\"}\n",
    "\n",
    "admissions_table = 'physionet-data.mimiciii_clinical.admissions'  # @param {type: \"string\"}\n",
    "d_icd_diagnoses_table = 'physionet-data.mimiciii_clinical.d_icd_diagnoses'  # @param {type: \"string\"}\n",
    "diagnoses_icd_table = 'physionet-data.mimiciii_clinical.diagnoses_icd'  # @param {type: \"string\"}\n",
    "patients_table = 'physionet-data.mimiciii_clinical.patients'  # @param {type: \"string\"}\n",
    "\n",
    "# Preprocess queries made with the %%bigquery magic\n",
    "# by substituting these values\n",
    "sub_dict = {\n",
    "    'admissions_table': admissions_table,\n",
    "    'd_icd_diagnoses_table': d_icd_diagnoses_table,\n",
    "    'diagnoses_icd_table': diagnoses_icd_table,\n",
    "    'patients_table': patients_table\n",
    "}\n",
    "\n",
    "# Get a suffix to attach to the names of the models created during this tutorial\n",
    "# to avoid collisions between simultaneous users.\n",
    "account = subprocess.check_output(\n",
    "    ['gcloud', 'config', 'list', 'account', '--format',\n",
    "     'value(core.account)']).decode().strip()\n",
    "sub_dict['suffix'] = re.sub(r'[^\\w]', '_', account)[:900]\n",
    "\n",
    "# Set the default project for running queries\n",
    "bigquery.magics.context.project = analysis_project\n",
    "\n",
    "# Set up the substitution preprocessing injection\n",
    "if bigquery.magics._run_query.func_name != 'format_and_run_query':\n",
    "  original_run_query = bigquery.magics._run_query\n",
    "\n",
    "def format_and_run_query(client, query, job_config=None):\n",
    "  query = query.format(**sub_dict)\n",
    "  return original_run_query(client, query, job_config)\n",
    "\n",
    "bigquery.magics._run_query = format_and_run_query\n",
    "\n",
    "print('analysis_project:', analysis_project)\n",
    "print()\n",
    "print('custom %%bigquery magic substitutions:')\n",
    "for k, v in sub_dict.items():\n",
    "  print(' ', '{%s}' % k, '→', v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_CdtZMKDnPls"
   },
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'svg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HXWP8XwMdTkb"
   },
   "outputs": [],
   "source": [
    "bq = bigquery.Client(project=analysis_project)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2IzDVuXwlUgx"
   },
   "source": [
    "## Case complexity & mortality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CATciGNqlgbt"
   },
   "source": [
    "This tutorial is a case study. We're going to use BQML and MIMIC3 to answer a\n",
    "research question.\n",
    "\n",
    "> _In the intensive care unit, are complex cases more or less likely to be\n",
    "> fatal?_\n",
    "\n",
    "Maybe it's obvious that they would be more fatal. After all, things only get\n",
    "worse as you add more comorbidities. Or maybe the exact opposite is true.\n",
    "Compare the patient who comes to the ICU with ventricular fibrillation to the\n",
    "patient who comes with a laundry list of chronic comorbidities. Especially\n",
    "within the context of a particular admission, the single acute condition seems\n",
    "more lethal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j02wTJnmfUGS"
   },
   "source": [
    "### Taking a first look at the data\n",
    "\n",
    "Do we have the data to answer this question?\n",
    "\n",
    "If you browse through the\n",
    "[list of tables in the MIMIC dataset](https://mimic.physionet.org/mimictables/admissions/),\n",
    "you'll find that whether the patient passed away during the course of their\n",
    "admission is recorded. We can also operationalize the definition of case\n",
    "complexity by counting the number of diagnoses that the patient had during an\n",
    "admission. More diagnoses means greater case complexity.\n",
    "\n",
    "We need to check that we have a sufficiently diverse sample to build a viable\n",
    "model. First we'll check our dependent variable, which measures whether a\n",
    "patient passed away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sSOidA_PfDNc"
   },
   "outputs": [],
   "source": [
    "%%bigquery\n",
    "SELECT\n",
    "  COUNT(*) as total,\n",
    "  SUM(HOSPITAL_EXPIRE_FLAG) as died\n",
    "FROM\n",
    "  `{admissions_table}`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pLbPacvifcdi"
   },
   "source": [
    "Clearly the ICU is a very serious place: about 10% of admissions are mortal. As\n",
    "data scientists, this tells us that we have a significant, albeit imbalanced,\n",
    "number of samples in both categories. The models we're training will easily\n",
    "adapt to this class imbalance, but we will need to be cautious when evaluating\n",
    "the performance of our models. After all, a model that simply says \"no one dies\"\n",
    "will be right 91% of the time.\n",
    "\n",
    "Next we'll look at the distribution of our independent variable: the number of\n",
    "diagnoses assigned to a patient during their admission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7wu0v5F6um9a"
   },
   "outputs": [],
   "source": [
    "%%bigquery hist_df\n",
    "SELECT\n",
    "  n_diagnoses, COUNT(*) AS cnt\n",
    "FROM (\n",
    "  SELECT\n",
    "    COUNT(*) AS n_diagnoses\n",
    "  FROM\n",
    "    `{diagnoses_icd_table}`\n",
    "  GROUP BY\n",
    "    HADM_ID\n",
    ")\n",
    "GROUP BY n_diagnoses\n",
    "ORDER BY n_diagnoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WAES-ZtlxbRO"
   },
   "outputs": [],
   "source": [
    "g = sns.barplot(\n",
    "    x=hist_df.n_diagnoses, y=hist_df.cnt, color=sns.color_palette()[0])\n",
    "# Remove every fifth label on the x-axis for readability\n",
    "for i, label in enumerate(g.get_xticklabels()):\n",
    "  if i % 5 != 4 and i != 0:\n",
    "    label.set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aC02HF1pjEBf"
   },
   "source": [
    "With the exception of the dramatic mode¹, the spread of the diagnosis counts is\n",
    "bell-curved shaped. The mathematical explanation of this is called central limit\n",
    "theorem. While this is by no means a deal breaker, the thins tails we see in the\n",
    "distribution can be a challenge for linear-regression models. This is because\n",
    "the extreme points tend to affect the\n",
    "[likelihood](https://en.wikipedia.org/wiki/Likelihood_function) the most, so\n",
    "having fewer of them makes your model more sensitive to outliers. Regularization\n",
    "can help with this, but if it becomes too much of a problem we can consider a\n",
    "different type of model (such as support-vector machines, or robust regression)\n",
    "instead of generalized linear regression.\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "\n",
    "¹ Which is sort of fascinating. Comparing the most common diagnoses for\n",
    "admissions with exactly 9 diagnoses to the rest of the cohort seems to suggest\n",
    "that this is due to positive correlations between cardiac diagnoses, e.g.\n",
    "cardiac complications NOS, mitral valve disorders, aortic valve disorders,\n",
    "subendocardial infarction etc. Your team might be interested in investigating\n",
    "this more seriously, especially if there is a cardiologist among you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OABwzKZAjXJI"
   },
   "source": [
    "### Creating a classification model\n",
    "\n",
    "[Creating a model with BigQuery ML](https://cloud.google.com/bigquery/docs/reference/standard-sql/bigqueryml-syntax-create)\n",
    "is simple. You write a normal query in standard SQL, and each row of the result\n",
    "is used as an input to train your model. BigQuery ML automatically applies the\n",
    "required\n",
    "[transformations](https://cloud.google.com/bigquery/docs/reference/standard-sql/bigqueryml-syntax-create#input_variable_transformations)\n",
    "depending on each variable's data type. For example, `STRING`s are transformed\n",
    "into [one-hot](https://en.wikipedia.org/wiki/One-hot) vectors, and `TIMESTAMP`s\n",
    "are\n",
    "[standardized](https://en.wikipedia.org/wiki/Feature_scaling#Standardization).\n",
    "These transformations are necessary to get a valid result, but they're easy to\n",
    "forget and a pain to implement. Without BQML, you also have to remember to apply\n",
    "these transformations when you make predictions and plots. It's fantastic that\n",
    "BigQuery takes care of all this for you.\n",
    "\n",
    "BigQuery ML also automatically performs\n",
    "[validation-based early stopping](https://en.wikipedia.org/wiki/Early_stopping#Validation-based_early_stopping)\n",
    "to prevent\n",
    "[overfitting](https://developers.google.com/machine-learning/glossary/#overfitting).\n",
    "\n",
    "To start, we're going to create a\n",
    "[(regularized) logistic regression](https://developers.google.com/machine-learning/crash-course/logistic-regression/)\n",
    "model that uses a single variable, the number of diagnoses a patient had during\n",
    "an admission, to predict the probability that a patient will pass away during an\n",
    "ICU admission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yddlrRwqhxqz"
   },
   "outputs": [],
   "source": [
    "%%bigquery\n",
    "# BigQuery ML create model statement:\n",
    "CREATE OR REPLACE MODEL `mimic_models.complexity_mortality_{suffix}`\n",
    "OPTIONS(\n",
    "  # Use logistic_reg for discrete predictions (classification) and linear_reg\n",
    "  # for continuous predictions (forecasting).\n",
    "  model_type = 'logistic_reg',\n",
    "  # See the below aside (𝜎 = 0.5 ⇒ 𝜆 = 2)\n",
    "  l2_reg = 2,\n",
    "  # Identify the column to use as the label (dependent variable)\n",
    "  input_label_cols = [\"died\"]\n",
    ")\n",
    "AS\n",
    "# standard SQL query to train the model with:\n",
    "SELECT\n",
    "  COUNT(*) AS number_of_diagnoses,\n",
    "  MAX(HOSPITAL_EXPIRE_FLAG) as died\n",
    "FROM\n",
    "  `{admissions_table}`\n",
    "  INNER JOIN `{diagnoses_icd_table}`\n",
    "  USING (HADM_ID)\n",
    "GROUP BY HADM_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f6dKkvFPkiTT"
   },
   "source": [
    "#### Optional aside: picking the regularization penalty $(\\lambda)$ with Bayes' Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zLN_zUUcEoV5"
   },
   "source": [
    "From the frequentist point of view,\n",
    "[$l_2$ regularized regression](https://developers.google.com/machine-learning/crash-course/regularization-for-simplicity/l2-regularization)\n",
    "minimizes the negative log-likelihood of a model with an added penalty term:\n",
    "$\\lambda \\| w \\|^2$. This penalty term reflects our desire for the model to be\n",
    "as simple as possible, and it removes the degeneracies caused by\n",
    "[collinear](https://en.wikipedia.org/wiki/Multicollinearity) input variables.\n",
    "\n",
    "$\\lambda$ is called `l2_reg` in BigQuery ML model options. You're given the\n",
    "freedom to set it to anything you want. In general, larger values of lambda\n",
    "encourage the model to give simpler explanations¹, and smaller values give the\n",
    "model more freedom to match the observed data. So what should you set $\\lambda$\n",
    "(a.k.a `l2_reg`) to?\n",
    "\n",
    "A short calculation (see e.g. chapters 4.3.2 and 4.5.1 of\n",
    "[Pattern Recognition and Machine Learning](https://books.google.ca/books?id=kOXDtAEACAAJ))\n",
    "shows that $l_2$ penalized logistic regression is equivalent to Bayesian\n",
    "logistic regression with the pior $ \\omega \\sim \\mathcal{N}(0, \\sigma^2 =\n",
    "\\frac{1}{2 \\lambda})$.\n",
    "\n",
    "Later on in this tutorial, we'll run an\n",
    "[$l_1$ regularized regression](https://developers.google.com/machine-learning/crash-course/regularization-for-sparsity/l1-regularization),\n",
    "which means the penalty term is $\\lambda \\| \\omega \\|$. The same reasoning\n",
    "applies except the corresponding prior is $w \\sim \\text{Laplace}(0, b =\n",
    "\\frac{1}{\\lambda})$.\n",
    "\n",
    "This Bayesian perspective gives meaning to the value of $\\lambda$. It reflects\n",
    "our prior uncertainty towards the strength of the relationship that we're\n",
    "modeling.\n",
    "\n",
    "Since BQML automatically standardizes and one-hot encodes its inputs, we can use\n",
    "this interpretation to give some generic advice on choosing $\\lambda$. If you\n",
    "don't have any special information, then any value of $\\lambda$ around $1$ is\n",
    "reasonable, and reflects that even a perfect correlation between the input and\n",
    "the output is not too surprising.\n",
    "\n",
    "As long as you choose $\\lambda$ to be much less than your sample size, its exact\n",
    "value should not influence your results very much. And even very small values of\n",
    "$\\lambda$ can remedy problems due to collinear inputs.\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "\n",
    "¹ Although regularization helps with overfitting, it does not completely solve\n",
    "it, and due care should still be taken not to select too many inputs for too\n",
    "little data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "73Z0dBnTE524"
   },
   "source": [
    "### Plotting the predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OQ3heEZRbdUC"
   },
   "source": [
    "We can inspect the weights that our model learned using the\n",
    "[`ML.WEIGHTS`](https://cloud.google.com/bigquery/docs/reference/standard-sql/bigqueryml-syntax-weights)\n",
    "statement. The positive weight that we see for `number_of_diagnoses` is our\n",
    "first evidence that case complexity is associated with mortality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nx_zpuDwi1x5"
   },
   "outputs": [],
   "source": [
    "%%bigquery simple_model_weights\n",
    "SELECT * FROM ML.WEIGHTS(MODEL `mimic_models.complexity_mortality_{suffix}`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ecy9VGWldzS7"
   },
   "source": [
    "By default the weights are automatically translated to their unstandardized\n",
    "forms. Meaning that we don't have to standardize our inputs before multiplying\n",
    "them with the weights to obtain predictions. You can see the standardized\n",
    "weights with `ML.WEIGHTS(MODEL ..., STRUCT(true AS standardize))`, which can be\n",
    "helpful for answering questions about the relative importance of different\n",
    "variables, regardless of their scale.\n",
    "\n",
    "We can use the unstandardized weights to make a python function that returns the\n",
    "predicted probability of mortality given an ICU admission with a certain number\n",
    "of diagnoses\n",
    "\n",
    "```python\n",
    "def predict(number_of_diagnoses):\n",
    "  return scipy.special.expit(\n",
    "    simple_model_weights.weight[0] * number_of_diagnoses\n",
    "    + simple_model_weights.weight[1])\n",
    "```\n",
    "\n",
    "but it's often faster and easier to make predictions with the\n",
    "[`ML.PREDICT`](https://cloud.google.com/bigquery/docs/reference/standard-sql/bigqueryml-syntax-predict)\n",
    "statement.\n",
    "\n",
    "We'd like to create a plot showing our model's predictions and the underlying\n",
    "data. We can use `ML.PREDICT` to get the data to draw the prediction line, and\n",
    "copy-paste the query we fed into `CREATE MODEL` to get the data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mk2_TiI5tbeE"
   },
   "outputs": [],
   "source": [
    "params = {'max_prediction': hist_df.n_diagnoses.max()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bs6_OdELQq-n"
   },
   "outputs": [],
   "source": [
    "%%bigquery line_df --params $params\n",
    "SELECT * FROM\n",
    "ML.PREDICT(MODEL `mimic_models.complexity_mortality_{suffix}`, (\n",
    "  SELECT * FROM\n",
    "  UNNEST(GENERATE_ARRAY(1, @max_prediction)) AS number_of_diagnoses\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MlNvOyZ1uZ2-"
   },
   "outputs": [],
   "source": [
    "%%bigquery scatter_df\n",
    "SELECT\n",
    "  COUNT(*) AS num_diag,\n",
    "  MAX(HOSPITAL_EXPIRE_FLAG) as died\n",
    "FROM\n",
    "  `{admissions_table}` AS adm\n",
    "  INNER JOIN `{diagnoses_icd_table}` AS diag\n",
    "  USING (HADM_ID)\n",
    "GROUP BY HADM_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KAx8zSoaoqPi"
   },
   "outputs": [],
   "source": [
    "sns.regplot(\n",
    "    x='num_diag',\n",
    "    y='died',\n",
    "    data=scatter_df,\n",
    "    fit_reg=False,\n",
    "    x_bins=np.arange(1,\n",
    "                     scatter_df.num_diag.max() + 1))\n",
    "plt.plot(line_df.number_of_diagnoses,\n",
    "         line_df.predicted_died_probs.apply(lambda x: x[0]['prob']))\n",
    "plt.xlabel('Case complexity (number of diagnoses)')\n",
    "plt.ylabel('Probability of death during admission')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FSi289ncu6pM"
   },
   "source": [
    "Qualitatively, our model fits the data quite well, and the trend is pretty\n",
    "clear. We might be tempted to say we've proven that increasing case complexity\n",
    "increases the probability of death during an admission to the ICU. While we've\n",
    "provided some evidence of this, we haven't proven it yet.\n",
    "\n",
    "The biggest problem is we don't know if case complexity is causing the increase\n",
    "in deaths, or if is merely correlated with some other variables that affect the\n",
    "probability of death more directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pg9AQy00xfPf"
   },
   "source": [
    "## Adding a confounding variable\n",
    "\n",
    "Patient age is a likely candidate for a confounding variable that could be\n",
    "mediating the relationship between complexity and mortality. Patients generally\n",
    "accrue diagnoses as they age¹ and approach their life expectancy. By adding the\n",
    "patient's age to our model, we can see how much of the relationship between case\n",
    "complexity and mortality is explained the patient's age.\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "\n",
    "¹ Using the `CORR` standard SQL function, you can calculate that the Pearson\n",
    "correlation coeffiecient between age and number of diagnoses is $0.37$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NtqgN1FVxjx1"
   },
   "outputs": [],
   "source": [
    "%%bigquery\n",
    "CREATE OR REPLACE MODEL `mimic_models.complexity_age_mortality_{suffix}`\n",
    "OPTIONS(model_type='logistic_reg', l2_reg=2, input_label_cols=[\"died\"])\n",
    "AS\n",
    "SELECT\n",
    "  # MIMIC3 sets all ages over 89 to 300 to avoid the possibility of\n",
    "  # identification.\n",
    "  IF(DATETIME_DIFF(ADMITTIME, DOB, DAY)/365.25 < 200,\n",
    "     DATETIME_DIFF(ADMITTIME, DOB, DAY)/365.25,\n",
    "     # The life expectancy of a 90 year old is approximately 5 years according\n",
    "     # to actuarial tables. So we'll use 95 as the mean age of 90+'s\n",
    "     95) AS age,\n",
    "  num_diag,\n",
    "  died\n",
    "FROM\n",
    "  (SELECT\n",
    "    COUNT(*) AS num_diag,\n",
    "    MAX(HOSPITAL_EXPIRE_FLAG) as died,\n",
    "    ANY_VALUE(ADMITTIME) as ADMITTIME,\n",
    "    SUBJECT_ID\n",
    "  FROM\n",
    "    `{admissions_table}` AS adm\n",
    "    JOIN `{diagnoses_icd_table}` AS diag\n",
    "  USING (HADM_ID, SUBJECT_ID)\n",
    "  GROUP BY HADM_ID, SUBJECT_ID\n",
    "  )\n",
    "  JOIN `{patients_table}` AS patients\n",
    "  USING (SUBJECT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1fbWf3dWfqDb"
   },
   "source": [
    "When we investigate the weights for this model, we see the weight associated\n",
    "with the number of diagnoses is only slightly smaller now. This tells us that\n",
    "some of the effect we saw in the univariate model was due to the confounding\n",
    "influence of age, but most of it wasn't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xE2o7wsDQoIv"
   },
   "outputs": [],
   "source": [
    "%%bigquery\n",
    "SELECT * FROM ML.WEIGHTS(MODEL `mimic_models.complexity_age_mortality_{suffix}`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DOvG9ljdftjj"
   },
   "source": [
    "Another way to understand this relationship is to compare the effectiveness of\n",
    "the model with and without age as an input. This answers the question: given the\n",
    "number of diagnoses that a patient has received, how much extra information does\n",
    "their age give us? To be thorough, we could also include a model with just the\n",
    "patient's age. You can add a couple of code cells to this notebook and do this\n",
    "as an exercise if you're curious."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "moxWSHVj6PzN"
   },
   "source": [
    "### Plotting ROC and precision-recall curves\n",
    "\n",
    "One way to compare the effectiveness of binary classification models is with\n",
    "[ROC curves or a precision-recall curves](https://developers.google.com/machine-learning/crash-course/classification/video-lecture).\n",
    "\n",
    "Since ROC curves tend to appear overly optimistic when the data has a\n",
    "significant class imbalance, we're going to favour precision-recall curves in\n",
    "this tutorial. Precision-Recall curves plot the recall (which measures the\n",
    "model's performance on the positive samples)\n",
    "\n",
    "$$\n",
    "\\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives} +\n",
    "\\text{False Negatives}}\n",
    "$$\n",
    "\n",
    "against the precision (which measures the model's performance on the samples _it\n",
    "classified_ as positive examples)\n",
    "\n",
    "$$\n",
    "\\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives} +\n",
    "\\text{False Positives}}\n",
    "$$\n",
    "\n",
    "as the [decision threshold](https://en.wikipedia.org/wiki/Decision_boundary)\n",
    "ranges from $0$ (predict no one dies) to $1$ (predict everyone dies)¹.\n",
    "\n",
    "To make these plots, we're going to use the\n",
    "[`ML.ROC_CURVE`](https://cloud.google.com/bigquery/docs/reference/standard-sql/bigqueryml-syntax-roc)\n",
    "BigQuery ML statement. `ML.ROC_CURVE` returns the data you need to draw both ROC\n",
    "and precision-recall curves with your graphing library of choice.\n",
    "\n",
    "`ML.ROC_CURVE` defaults to using data from the evaluation dataset. If it\n",
    "operated on the training dataset, it would be difficult to distinguish\n",
    "overfitting from excellent performance. If you have your own validation dataset,\n",
    "you can provide it as an optional second argument.\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "\n",
    "¹ BigQuery ML uses the convention that the threshold is between $0$ and $1$,\n",
    "rather than the logit of this value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K9zceou0e18T"
   },
   "outputs": [],
   "source": [
    "%%bigquery comp_roc\n",
    "SELECT * FROM ML.ROC_CURVE(MODEL `mimic_models.complexity_mortality_{suffix}`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zARPYfxV3v3s"
   },
   "outputs": [],
   "source": [
    "%%bigquery comp_age_roc\n",
    "SELECT * FROM\n",
    "ML.ROC_CURVE(MODEL `mimic_models.complexity_age_mortality_{suffix}`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3vSu2lp1FwhY"
   },
   "outputs": [],
   "source": [
    "def set_precision(df):\n",
    "  df['precision'] = df.true_positives / (df.true_positives + df.false_positives)\n",
    "\n",
    "\n",
    "def plot_precision_recall(df, label=None):\n",
    "  # manually add the threshold = -∞ point\n",
    "  df = df[df.true_positives != 0]\n",
    "  recall = [0] + list(df.recall)\n",
    "  precision = [1] + list(df.precision)\n",
    "  # x=recall, y=precision line chart\n",
    "  plt.plot(recall, precision, label=label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fHFHY0z6fBtP"
   },
   "outputs": [],
   "source": [
    "set_precision(comp_roc)\n",
    "set_precision(comp_age_roc)\n",
    "plot_precision_recall(comp_age_roc, label='bivariate (age) model')\n",
    "plot_precision_recall(comp_roc, label='univariate model')\n",
    "plt.plot(\n",
    "    np.linspace(0, 1, 2), [comp_roc.precision.min()] * 2,\n",
    "    label='null model',\n",
    "    linestyle='--')\n",
    "plt.legend()\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.xlabel(r'Recall $\\left(\\frac{T_p}{T_p + F_n} \\right)$')\n",
    "plt.ylabel(r'Precision $\\left(\\frac{T_p}{T_p + F_p} \\right)$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8G0tmhz3Q9KY"
   },
   "source": [
    "We see that:\n",
    "\n",
    "*   Both these models are significantly better than the zero variable model,\n",
    "    implying that case complexity has a significant impact on patient mortality.\n",
    "*   Adding the patient's age only marginally improves the model, implying that\n",
    "    the impact of case complexity is not mediated through age.\n",
    "\n",
    "Of course, neither of these models is very good when it comes to making\n",
    "predictions. For our last set of models, we'll try more earnestly to predict\n",
    "patient mortality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o52gB0FEVCw_"
   },
   "source": [
    "## More complex models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ywpoyAiraVnp"
   },
   "source": [
    "One of the main attractions of BigQuery ML is its ability to scale to high\n",
    "dimensional models with\n",
    "[up to millions of variables](https://cloud.google.com/bigquery/docs/reference/standard-sql/bigqueryml-syntax-create#create_model_limitations).\n",
    "Our dataset isn't nearly large enough to train this many variables without\n",
    "severe overfitting, be we can still abide training models with hundreds of\n",
    "variables.\n",
    "\n",
    "Our strategy will use the $m$ most frequent diagnoses, and a handful of other\n",
    "likely relevant variables as the inputs to our model. Namely, we'll use:\n",
    "\n",
    "*   `ADMISSION_TYPE`: reflects the reason for, and seriousness of the admission\n",
    "    *   urgent\n",
    "    *   emergency\n",
    "    *   newborn\n",
    "    *   elective\n",
    "*   `INSURANCE`: reflects the patients socioeconomic status, a well-known\n",
    "    covariate with patient outcomes\n",
    "    *   Self Pay\n",
    "    *   Medicare\n",
    "    *   Private\n",
    "    *   Medicaid\n",
    "    *   Government\n",
    "*   `GENDER`: accounts for both social and physiological differences across\n",
    "    genders\n",
    "*   `AGE`: accounts for both social and physiological differences across ages\n",
    "*   number of diagnoses: our stand-in for case complexity\n",
    "\n",
    "in addition to the top $m$ diagnoses. We'll compare models with $m \\in \\left\\{8,\n",
    "16, 32, 64, 128, 256, 512 \\right\\}$ to determine the most sensible value of $m$.\n",
    "\n",
    "This will give us valuable information regarding our original question: whether\n",
    "case complexity increases the probability of ICU mortality. We wonder if the\n",
    "number of diagnoses increases patient risk only because it increases the chances\n",
    "of one of their many diagnoses being lethal, or if these is an interactive\n",
    "effect¹. We'll be able to test this by determining whether\n",
    "$\\omega_{n_{\\text{diagnoses}}}$ goes to $0$ as we increase $m$.\n",
    "\n",
    "We'll also get some interesting information on the relative lethality of\n",
    "different diagnoses, and how these compare with social determinants.\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "\n",
    "¹As in the often misattributed quote:\n",
    "[quantity has a quality all its own](https://en.wikiquote.org/wiki/Quantity), or\n",
    "does it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "up-C-eU-6SAX"
   },
   "source": [
    "### Creating the models\n",
    "\n",
    "We'll start by getting a list of the most frequent diagnoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SwEIqaBsVG0y"
   },
   "outputs": [],
   "source": [
    "%%bigquery top_diagnoses\n",
    "WITH top_diag AS (\n",
    "  SELECT COUNT(*) AS count, ICD9_CODE FROM `{diagnoses_icd_table}`\n",
    "  GROUP BY ICD9_CODE\n",
    ")\n",
    "SELECT top_diag.ICD9_CODE, icd_lookup.SHORT_TITLE, top_diag.count FROM\n",
    "top_diag JOIN\n",
    " `{d_icd_diagnoses_table}` AS icd_lookup\n",
    "USING (ICD9_CODE)\n",
    "ORDER BY count DESC LIMIT 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RyoY8k5r7Cdt"
   },
   "source": [
    "which we'll use to create our models. In the `CREATE MODEL` SELECT statement, we\n",
    "create one column for each of the $m$ diagnoses and fill it with $1$ if the\n",
    "patient had that diagnosis and $0$ otherwise.\n",
    "\n",
    "This time around we're using `l1_reg` instead of `l2_reg` because we expect that\n",
    "some of our some of our many variables will not significantly impact the\n",
    "outcome, and we would prefer a sparse model if possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nKU3_dQ5O1DO"
   },
   "outputs": [],
   "source": [
    "top_n_diagnoses = (8, 16, 32, 64, 128, 256, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sqITDZkNcOgf"
   },
   "outputs": [],
   "source": [
    "query_jobs = list()\n",
    "for m in top_n_diagnoses:\n",
    "  # The expressions for creating the new columns for each input diagnosis\n",
    "  diagnosis_columns = list()\n",
    "  for _, row in top_diagnoses.iloc[:m].iterrows():\n",
    "    diagnosis_columns.append('MAX(IF(ICD9_CODE = \"{0}\", 1.0, 0.0))'\n",
    "                             ' as `icd9_{0}`'.format(row.ICD9_CODE))\n",
    "\n",
    "  query = \"\"\"\n",
    "  CREATE OR REPLACE MODEL `mimic_models.predict_mortality_diag_{m}_{suffix}`\n",
    "  OPTIONS(model_type='logistic_reg', l1_reg=2, input_label_cols=[\"died\"])\n",
    "  AS\n",
    "  WITH diagnoses AS (\n",
    "    SELECT\n",
    "      HADM_ID,\n",
    "      COUNT(*) AS num_diag,\n",
    "      {diag_cols}\n",
    "    FROM `{diagnoses_icd_table}`\n",
    "    WHERE ICD9_CODE IS NOT NULL\n",
    "    GROUP BY HADM_ID\n",
    "  )\n",
    "  SELECT\n",
    "    IF(DATETIME_DIFF(adm.ADMITTIME, patients.DOB, DAY)/365.25 < 200,\n",
    "       DATETIME_DIFF(adm.ADMITTIME, patients.DOB, DAY)/365.25, 95) AS age,\n",
    "    diagnoses.* EXCEPT (HADM_ID),\n",
    "    adm.HOSPITAL_EXPIRE_FLAG as died,\n",
    "    adm.ADMISSION_TYPE as adm_type,\n",
    "    adm.INSURANCE as insurance,\n",
    "    patients.GENDER\n",
    "  FROM\n",
    "    `{admissions_table}` AS adm\n",
    "    LEFT JOIN `{patients_table}` AS patients USING (SUBJECT_ID)\n",
    "    LEFT JOIN diagnoses USING (HADM_ID)\n",
    "  \"\"\".format(\n",
    "      m=m, diag_cols=',\\n    '.join(diagnosis_columns), **sub_dict)\n",
    "  # Run the query, and track its progress with query_jobs\n",
    "  query_jobs.append(bq.query(query))\n",
    "\n",
    "# Wait for all of the models to finish training\n",
    "for j in query_jobs:\n",
    "  j.exception()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qr0najss4mqU"
   },
   "source": [
    "### Getting evaluation metrics\n",
    "\n",
    "To obtain numerical evaluation metrics on your models, BigQuery ML provides the\n",
    "[`ML.EVALUATE`](https://cloud.google.com/video-intelligence/docs/common/auth)\n",
    "statement. Just like `ML.ROC_CURVE`, `ML.EVALUATE` defaults to using the\n",
    "evaluation dataset that was set aside when the model was created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sH743COCJaaH"
   },
   "outputs": [],
   "source": [
    "eval_queries = list()\n",
    "for m in top_n_diagnoses:\n",
    "  eval_queries.append(\n",
    "      'SELECT * FROM ML.EVALUATE('\n",
    "      'MODEL `mimic_models.predict_mortality_diag_{}_{suffix}`)'\n",
    "      .format(m, **sub_dict))\n",
    "eval_query = '\\nUNION ALL\\n'.join(eval_queries)\n",
    "bq.query(eval_query).result().to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mf0AnRD7EZOB"
   },
   "source": [
    "And we can also plot the precision-recall curves as we did before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GkcE9tR-dn8-"
   },
   "outputs": [],
   "source": [
    "for m in top_n_diagnoses:\n",
    "  df = bq.query('SELECT * FROM ML.ROC_CURVE('\n",
    "                'MODEL `mimic_models.predict_mortality_diag_{}_{suffix}`)'\n",
    "                .format(m, **sub_dict)).result().to_dataframe()\n",
    "  set_precision(df)\n",
    "  plot_precision_recall(df, label='{} diagnoses'.format(m))\n",
    "\n",
    "plt.plot(\n",
    "    np.linspace(0, 1, 2), [df.precision.min()] * 2,\n",
    "    label='null model',\n",
    "    linestyle='--')\n",
    "plt.legend()\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.xlabel(r'Recall $\\left(\\frac{T_p}{T_p + F_n} \\right)$')\n",
    "plt.ylabel(r'Precision $\\left(\\frac{T_p}{T_p + F_p} \\right)$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KDbvcXuqJWxN"
   },
   "source": [
    "The model with $m = 512$ seems to be overfitting the data, while somewhere\n",
    "between $m = 128$ and $m = 256$ seems to be the sweet spot for model\n",
    "flexibility. Since we've now used the evaluation dataset to determine $m$\n",
    "(albeit informally), and when to stop-early during training, dogmatic rigour\n",
    "would demand that we measure our model on a third (validation) dataset before we\n",
    "brag about its efficacy. On the other hand, there isn't a ton of flexibility in\n",
    "choosing between a few different values of $m$, nor in when to stop early. You\n",
    "can use your own judgment.\n",
    "\n",
    "Actually, the predictive power of our model¹ isn't nearly as interesting as it's\n",
    "weights and what they tell us. In the next section, we'll dig into them.\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "\n",
    "¹Which could be described as approaching respectability, but still a long way\n",
    "away from brag worthy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E-JEGEIs4xyM"
   },
   "source": [
    "### Exploring our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A3r9Cx08HMVd"
   },
   "source": [
    "Let's have a look at the weights from the $m = 128$ model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Aere0NKIYOf5"
   },
   "outputs": [],
   "source": [
    "%%bigquery weights_128\n",
    "SELECT * FROM ML.WEIGHTS(MODEL `mimic_models.predict_mortality_diag_128_{suffix}`)\n",
    "ORDER BY weight DESC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nsjiyxVvQTqg"
   },
   "source": [
    "First we'll look at the weights for the numerical inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "979wQy5-wbPa"
   },
   "outputs": [],
   "source": [
    "pd.set_option('max_rows', 150)\n",
    "weights_128['ICD9_CODE'] = weights_128.processed_input \\\n",
    "  .apply(lambda x: x[len('icd9_'):] if x.startswith('icd9_') else x)\n",
    "view_df = weights_128.merge(top_diagnoses,how='left', on='ICD9_CODE') \\\n",
    "  .rename(columns={'ICD9_CODE': 'input'})\n",
    "view_df = view_df[~pd.isnull(view_df.weight)]\n",
    "view_df[['input', 'SHORT_TITLE', 'weight', 'count']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ab5CO54YRsRl"
   },
   "source": [
    "We see have a list of diagnoses, sorted from most fatal to least fatal according\n",
    "to our model.\n",
    "\n",
    "Going back to our original question, we can see that the weight for `num_diag`\n",
    "(a.k.a the number of diagnoses) has essentially gone to zero. The average\n",
    "diagnoses weight is also very small:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XQG2YcFJTh_I"
   },
   "outputs": [],
   "source": [
    "view_df[~pd.isnull(view_df.SHORT_TITLE)].weight.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rmOeQjDhUD-3"
   },
   "source": [
    "so we can conclude that given that a patient has been admitted to the ICU, the\n",
    "number of diagnoses they've been given does not predict their outcome beyond the\n",
    "linear effect of the component diagnoses.\n",
    "\n",
    "It might be surprising that the weight for age is also very small. One\n",
    "explanation for this might be that DNR¹ status, and falls are among the highest\n",
    "weighted diagnoses. These diagnoses are associated with advanced age² ³ and\n",
    "there is literature³ to support that DNR status mediates the effect of age on\n",
    "survival. One thing we couldn't find much data on was the relationship between\n",
    "age and palliative treatment. This could be a good subject for a datathon team\n",
    "to tackle.\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "\n",
    "¹Do not resuscitate\n",
    "\n",
    "²[Article: Age-Related Changes in Physical Fall Risk Factors: Results from a 3\n",
    "Year Follow-up of Community Dwelling Older Adults in Tasmania,\n",
    "Australia](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3863882/)\n",
    "\n",
    "³[Article: Do Not Resuscitate (DNR) Status, Not Age, Affects Outcomes after\n",
    "Injury: An Evaluation of 15,227 Consecutive Trauma\n",
    "Patients](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3634122/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j6HdaYtmawum"
   },
   "source": [
    "Now let's look at the weights for the categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lmlSzG55cCz3"
   },
   "outputs": [],
   "source": [
    "for _, row in weights_128[pd.isnull(weights_128.weight)].iterrows():\n",
    "  print(row.processed_input)\n",
    "  print(\n",
    "      *sorted([tuple(x.values()) for x in row.category_weights],\n",
    "              key=lambda x: x[1],\n",
    "              reverse=True),\n",
    "      sep='\\n',\n",
    "      end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UmFB3GrRdLpR"
   },
   "source": [
    "We see that the patient's insurance has a startlingly large effect in our model.\n",
    "\n",
    "For those of us not familiar with american medical insurance terminology¹:\n",
    "\n",
    "*   Self pay: the patient pays out-of-pocket for their medical care as they\n",
    "    require it\n",
    "*   Medicare: a government program for people with low incomes\n",
    "*   Private: insurance that is usually paid for by the patient's employer\n",
    "*   Medicaid: a government program for people who have a disability or are over\n",
    "    65 years old\n",
    "*   Government: insurance granted by the government excluding medicare and\n",
    "    medicaid. This includes government employees and veterans.\n",
    "\n",
    "The impact of socioeconomic status on health is on clear display here. The\n",
    "difference between the weights for medicare and private insurance is $0.25$,\n",
    "which is similar to the weight for atrial fibrillation.\n",
    "\n",
    "The outlook for patient's paying out of pocket is also grim, and may reflect an\n",
    "avoidance of hospital care for financial reasons in addition to other\n",
    "socioeconomic factors.\n",
    "\n",
    "The weights for the admission type seem to reflect common sense, as do the\n",
    "weights for gender given that females have a longer life expectancy than males.\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "\n",
    "¹ See https://en.wikipedia.org/wiki/Health_insurance_in_the_United_States\n",
    "\n",
    "² There are thousands of articles on this, see e.g. [Article: Socioeconomic\n",
    "Disparities in Health in the United States: What the Patterns Tell\n",
    "Us](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2837459/)\n",
    "\n",
    "³ See https://en.wikipedia.org/wiki/List_of_countries_by_life_expectancy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WHkTRwmpQtXI"
   },
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4XnDmbljQ3eR"
   },
   "source": [
    "We've found evidence that case complexity increases the risk of an ICU\n",
    "admission, but only through the cumulative effects of the component diagnoses.\n",
    "That's not to say that these nonlinear interactions aren't very powerful in\n",
    "certain cases¹, but that this seems to be the exception rather than the rule.\n",
    "\n",
    "We were able to obtain these results entirely from within BigQuery, with minimal\n",
    "modifications to standard SQL statements, only resorting to python for\n",
    "visualization.\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "\n",
    "¹ That is, between certain combinations or cliques of diagnoses."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "BigQuery_ML.ipynb",
   "provenance": [
    {
     "file_id": "/piper/depot/google3/third_party/cloud/healthcare/datathon/mimic_eicu/tutorials/BigQuery_ML.ipynb",
     "timestamp": 1557276874814
    },
    {
     "file_id": "1E9Xw-isoxDW9Sf8Mj9VigvtpM0RCNowv",
     "timestamp": 1548428320183
    },
    {
     "file_id": "1QTQQ1Y1wifsXXC9KAqO6URZuAbuRSW4_",
     "timestamp": 1548275858698
    },
    {
     "file_id": "1qT4NHyVPLzvBbXmifgc6McG15GvOzysY",
     "timestamp": 1548110649398
    }
   ],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
